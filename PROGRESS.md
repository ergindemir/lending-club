# Daily Progress

## 01/21-22
* Download Dataset
* Create github repo
* Study data dictionary
* Study pickling techniques and performance gain
* Basic EDA and varibale transformations

## 01/23
* Detailed EDA
* Filter columns with too many missing variables
* Split the dataset into continous and categorical variables
* Transform the dependent variable
* Fill the missing valules

## 01/24
* Evaluate the IV criterion for input variables
* Compile the dataset for modelling
* Encode categorical data for Random Forest and Logistic Regression seperately
* Transform variables engineer features
* Run test models and based on results:
  * Filter Columns that might carry leakage
  * Remove features with high correlation
  * Determine the most significant varibles

## 01/25
* Code added to generate confusion matrices and ROC curves
* Grid search process started
* FPR and TPR rates have been studied, it comes out that FN cases are extremely low, which is good for the business.
* The S&P 500 data for last 10 years have been downloaded
* Attended introductory course on Javascript, first application (a game) coded in js.
* Still searching for a good web template, it looks like admin dasboard templates are most suitable for this project.

## 01/26-30
* Start building classes and cleaning the code.
* Start working on time series data af SP500, generate volatility curve.
* More research on building a website with flask and bootstrap theme
* Start working on portfolio theory, in particular risk/return curves.
* Introduction to quadratic programming with python to optimize portfolio

## 01/31 (Planned)
* Using the previous compile three modules
  * A model that predicts the default rates based on the dataset
  * A model that combines volatility of the stock market for better prediction
  * A module that optimizes porfolio for a given risk level

