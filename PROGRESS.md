# Daily Progress

## 01/21-22
* Download Dataset
* Create github repo
* Study data dictionary
* Study pickling techniques and performance gain
* Basic EDA and varibale transformations

## 01/23
* Detailed EDA
* Filter columns with too many missing variables
* Split the dataset into continous and categorical variables
* Transform the dependent variable
* Fill the missing valules

## 01/24
* Evaluate the IV criterion for input variables
* Compile the dataset for modelling
* Encode categorical data for Random Forest and Logistic Regression seperately
* Transform variables engineer features
* Run test models and based on results:
  * Filter Columns that might carry leakage
  * Remove features with high correlation
  * Determine the most significant varibles

## 01/25
* Code added to generate confusion matrices and ROC curves
* Grid search process started
* FPR and TPR rates have been studied, it comes out that FN cases are extremely low, which is good for the business.
* The S&P 500 data for last 10 years have been downloaded
* Attended introductory course on Javascript, first application (a game) coded in js.
* Still searching for a good web template, it looks like admin dasboard templates are most suitable for this project.

## 01/26-27 (Planned)
* Further study of models. Iteratively remove dominant features and try to reproduce performance.
* Start building classes and cleaning the code.
* Start working on time series data af SP500, generate volatility curve.
* Determine how default rates change with time for a given credit grade.
* Try incorporate volatility data to the set and devise an ensemble model forbetter prediction.
* Start working on portfolio theory, in particular risk/return characteristics.


